<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: For Dummies | Adrian Mejia's Blog]]></title>
  <link href="http://adrianmejia.com/blog/categories/for-dummies/atom.xml" rel="self"/>
  <link href="http://adrianmejia.com/"/>
  <updated>2014-09-28T18:55:51-04:00</updated>
  <id>http://adrianmejia.com/</id>
  <author>
    <name><![CDATA[Adrian Mejia]]></name>
    <email><![CDATA[me@adrianmejia.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Algorithms for Dummies (Part 1): Big-O Notation and Sorting]]></title>
    <link href="http://adrianmejia.com/blog/2014/02/13/algorithms-for-dummies-part-1-sorting/"/>
    <updated>2014-02-13T09:28:51-04:00</updated>
    <id>http://adrianmejia.com/blog/2014/02/13/algorithms-for-dummies-part-1-sorting</id>
    <content type="html"><![CDATA[<p>After being developing software for a while,  I realized that there is a couple of ways to become better at it. One it&rsquo;s through your experience: writing code, working on projects, getting hands dirty&hellip; Other one it&rsquo;s learning algorithms and design patterns. In other words through leveraging the experience of other computer scientists. Learning to use algorithms efficiently can instantly add to you the equivalent of 10 years of experience or more. Let&rsquo;s get started and add new tools to our arsenal!</p>

<p>How do you know a piece of code that you just wrote is good enough?  When you modify a program, how do you know if it is better as you found it? How do scale programs to handle huge amount of data? In fact, You cannot improve what you can&rsquo;t measure.</p>

<p>How to measure them? We could count the number of seconds it takes to execute and compare it with other one. However, it&rsquo;s not just troublesome to timers around code but if we run it in different hardware (e.g. supercomputer) it will seem like more efficient when indeed it&rsquo;s exactly the same program. Let&rsquo;s illustrate a better way with an example. Let&rsquo;s say you want to sort an array of n integers.</p>

<h1>Sorting Algorithms</h1>

<pre><code class="java">  void sort(int[] arr){
    for(int x=1; x &lt; arr.length; x++)
      for(int y=x; y &gt; 0 &amp;&amp; arr[y-1] &gt; arr[y]; y--){
          int t = arr[y];
          arr[y] = arr[y-1];
          arr[y-1] = t;
        }
  }
</code></pre>

<p>Do you recognize this algorithm? It&rsquo;s called Insertion sort. It has two nested loops, which means that as the number of elements n in the array <code>arr</code> grows it will take approximately n * n longer to perform the sorting. In big-O notation, this will be represented like O(n<sup>2</sup>). More on this notation later.</p>

<p>What would happen if the array arr is already sorted? That would be the best-case scenario. The inner for loop will never go through all the elements in the array then (because <code>arr[y-1] &gt; arr[y]</code>  won&rsquo;t be met). So the algorithm in run in O(n).</p>

<p>We are not living in an ideal world. So O(n<sup>2</sup>) will be probably the average time complexity. Can you think a better way of sorting an array of elements?</p>

<p>Take some minutes to think and come back&hellip;</p>

<h2>Merge Sort</h2>

<p>A more efficient algorithm is the Merge sort. It uses the principle of divide and conquer to solve the problem faster. The idea is the follows:</p>

<p><img src="/images/mergesort.gif"></p>

<ul>
<li>Divide the array in half</li>
<li>Divide the halves by half until 2 or 3 elements are remaining</li>
<li>Sort each of these halves</li>
<li>Merge them back together</li>
</ul>


<p>Can you determine the time complexity of mergesort?</p>

<pre><code class="java">  void sort(int[] arr){
    int[] helper = new int[arr.length];
    mergesort(arr, helper, 0, arr.length-1);
  }

  void mergesort(int[] arr, int[] helper, int low, int high){
    if(low &lt; high){
      int middle = (high+low)/2;
      mergesort(arr, helper, low, middle);
      mergesort(arr, helper, middle+1, high);
      merge(arr, helper, low, middle, high);
    }
  }

  void merge(int[] arr, int[] helper, int low, int middle, int high){
    for (int x=low; x &lt;= high; x++) {
      helper[x] = arr[x];
    }

    int left = low;
    int curr = low;
    int right = middle+1;

    while(left &lt;= middle &amp;&amp; right &lt;= high) {
      if(helper[right] &gt; helper[left])
        arr[curr++] = helper[left++];
      else
        arr[curr++] = helper[right++];
    }

    while(left &lt;= middle)
      arr[curr++] = helper[left++];
  }
</code></pre>

<p>Even though the code is much longer, the algorithm is much more efficient.</p>

<p><img src="/images/insertion_vs_mergsort.png"></p>

<p>It would take some more knowledge to derive the running time mathematically, and we haven&rsquo;t covered that yet. However, bear with me, it&rsquo;s O(n log(n)). Let&rsquo;s sum up:</p>

<table>
<thead>
<tr>
<th>Algorithm </th>
<th> best </th>
<th> average </th>
<th> worst </th>
<th> space complexity</th>
</tr>
</thead>
<tbody>
<tr>
<td>Insertion Sort </td>
<td> O(n) </td>
<td> O(n<sup>2</sup>) </td>
<td> O(n<sup>2</sup>) </td>
<td> O(1)</td>
</tr>
<tr>
<td>Merge sort </td>
<td> O(n log(n)) </td>
<td> O(n log(n)) </td>
<td> O(n log(n)) </td>
<td> O(n)</td>
</tr>
</tbody>
</table>


<p>Notice that the table has also the space complexity. How much space does the algorithms take is also an important parameter to compare algorithms. The merge sort uses an additional array that&rsquo;s way its space complexity is <code>O(n)</code>, however, the insertion sort uses <code>O(1)</code> because it does the sorting in-place.</p>

<p>Big O Notation</p>

<p>Big O is defined as the asymptotic upper limit of a function. In plain english, it means that is a function that cover the maximum values a function could take. As we saw a little earlier this notation help us to predict performance and compare algorithms.</p>

<p></p>

<table border=1>
<tr><th>Growth Rate</th><th>Name</th></tr>
<tr><td>1</td><td>Constant</td></tr>
<tr><td>log(n)</td><td>Logarithmic</td></tr>
<tr><td>n</td><td>Linear</td></tr>
<tr><td>n*log(n)</td><td>Linearithmic</td></tr>
<tr><td>n^2</td><td>Quadratic</td></tr>
<tr><td>n^3</td><td>Cubic</td></tr>
<tr><td>2^n</td><td>Exponential</td></tr>
</table>


<p>This is kinda abstract let&rsquo;s see what it means in code:</p>

<table border=1>
<tr><td>Growth Rate</td><td>Name</td><td>Code e.g.</td><td>description</td></tr>
<tr><td>1</td><td>Constant</td><td>a+=1;</td><td>statement (one line of code)</td></tr>
<tr><td>log(n)</td><td>Logarithmic</td><td>
<pre>
while(n>1){
  n=n/2;
}
</pre>
</td><td>Divide in half (binary search)</td></tr>
<tr><td>n</td><td>Linear</td><td>
<pre>
for(c=0; c&lt;n; c++){
  a+=1;
}
</pre></td><td>Loop</td></tr>
<tr><td>n*log(n)</td><td>Linearithmic</td><td>Mergesort, Quicksort, &#8230;</td><td>Effective sorting algorithms</td></tr>
<tr><td>n^2</td><td>Quadratic</td><td>
<pre>
for(c=0; c&lt;n; c++){
  for(i=0; i&lt;n; i++){
    a+=1;
  }
}
</pre>
</td><td>Double loop</td></tr>
<tr><td>n^3</td><td>Cubic</td><td>
<pre>
for(c=0; c&lt;n; c++){
  for(i=0; i&lt;n; i++){
    for(x=0; x&lt;n; x++){
      a+=1;
    }
  }
}
</pre>
</td><td>Triple loop</td></tr>
<tr><td>2^n</td><td>Exponential</td><td>Trying to braeak a password generating all possible combinations</td><td>Exhaustive search</td></tr>
</table>


<p>That&rsquo;s all for this first part 1. I will continue publishing this tutorials every week or so. Stay tune!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Learning Algorithms From Scratch / Algorithms for Dummies]]></title>
    <link href="http://adrianmejia.com/blog/2011/12/22/learning-algorithms-from-scratch-algorithms-for-dummies/"/>
    <updated>2011-12-22T00:00:00-04:00</updated>
    <id>http://adrianmejia.com/blog/2011/12/22/learning-algorithms-from-scratch-algorithms-for-dummies</id>
    <content type="html"><![CDATA[<p>update: graphs are gone. Checkout my new post with new content and images: <a href="http://adrianmejia.com/blog/2014/02/13/algorithms-for-dummies-part-1-sorting/"><a href="http://adrianmejia.com/blog/2014/02/13/algorithms-for-dummies-part-1-sorting/">http://adrianmejia.com/blog/2014/02/13/algorithms-for-dummies-part-1-sorting/</a></a></p>

<p>When you are programming you face challenges all the way. Getting the problems solved is just the tip of the iceberg, getting it done efficiently is the rest.</p>


<p class="p1"><b>Why should you care for efficiency?</b></p>


<p class="p1">Solutions to the same problem might take years with certain algorithm, and just minutes using efficient algorithms. For instance, if you have applications that are used for thousands of people over internet, every fraction of second counts. Therefore, efficient algorithms is a must.</p>


<p class="p1"><b>How I do my algorithms more efficient?</b></p>


<p class="p1">To improve something you first need to know the actual state. In this case you need to measure the actual effectiveness of your algorithm in other to improve it. It&#39;s very common to use running time analysis to measure the speed of algorithms independently from the hardware used (old pc, supercomputer it doesn&#39;t matter).&nbsp;</p>


<p class="p1"><b>Run-time analysis</b></p>


<p class="p1">A common way to analyze the algorithms is using the big-O notation. The good thing about this notation is that is independent from the computer used to run the algorithm. You know that if you use a very slow computer (e.g. pentium I) v.s. a supercomputer use in NASA, the latter will run the program much faster. Big-O notation abstract the hardware and just focus in the algorithm per se. The only variable in the big-O notation gives the relative time needed to process an algorithm in function of the input n. Let&#39;s clarify this with an example.</p>


<p class="p1"><strong>Ex.1</strong> - You want to sort an array A of n integers.&nbsp;</p>


<p class="p1">Depending in the algorithm used to do that you may have:</p>


<ul>
    <li class="p1">
        <b>selection</b> sort has a running time of O(n^2);</li>
    <li class="p1">
        <b>merge sort</b> &#8211;&gt; O(n log n)</li>
</ul>


<p class="p1">Right now, it doesn&#39;t matter if are not familiar with these algorithms (we will cover this the next lessons), the point here is that we have n integer and big-O notations give us a mathematical expression that is in function of the input n. If you&nbsp;<a href="http://fooplot.com/index.php?&amp;type0=0&amp;type1=0&amp;type2=0&amp;type3=0&amp;type4=0&amp;y0=x%5E2&amp;y1=x*log%28x%29&amp;y2=&amp;y3=&amp;y4=&amp;r0=&amp;r1=&amp;r2=&amp;r3=&amp;r4=&amp;px0=&amp;px1=&amp;px2=&amp;px3=&amp;px4=&amp;py0=&amp;py1=&amp;py2=&amp;py3=&amp;py4=&amp;smin0=0&amp;smin1=0&amp;smin2=0&amp;smin3=0&amp;smin4=0&amp;smax0=2pi&amp;smax1=2pi&amp;smax2=2pi&amp;smax3=2pi&amp;smax4=2pi&amp;thetamin0=0&amp;thetamin1=0&amp;thetamin2=0&amp;thetamin3=0&amp;thetamin4=0&amp;thetamax0=2pi&amp;thetamax1=2pi&amp;thetamax2=2pi&amp;thetamax3=2pi&amp;thetamax4=2pi&amp;ipw=0&amp;ixmin=-5&amp;ixmax=5&amp;iymin=-3&amp;iymax=3&amp;igx=1&amp;igy=1&amp;igl=1&amp;igs=0&amp;iax=1&amp;ila=1&amp;xmin=-5&amp;xmax=5&amp;ymin=-3&amp;ymax=3"><span class="s1">plot in a graph n^2 and n log n</span></a>. You&#39;ll see that n^2 grows much faster than n log(n). That means that the algorithm n^2 will take longer than n*log(n) to process as the size of the array n increases.</p>


<p class="p1"><b>Common order of Growth</b></p>


<p class="p1">To give you an idea of the common order of growth of runtime expressions. Take a look at the following graph and table. The slower the function growth the better is the algorithm. In order from better performance to worst is:</p>


<p class="p1">1 &#8211; log n &#8211; n &#8211; n log n &#8211; n^2 &#8211; n^3 &#8211; 2^n &#8211; n! &#8230;</p>


<p class="p2"><img alt="" src="http://adrianmejiarosario.com/sites/default/files/Screen%20Shot%202011-12-22%20at%203.22.12%20PM.png" style="width: 300px; height: 306px; " /></p>


<p class="p2">&nbsp;</p>


<p class="p2"><img alt="" src="http://adrianmejiarosario.com/sites/default/files/Screen%20Shot%202011-12-22%20at%203.23.45%20PM.png" style="width: 300px; height: 257px; " /></p>


<p class="p2">&nbsp;</p>


<p class="p1">&nbsp;</p>


<p class="p1"><b>Approximate growth rate from code.</b></p>


<p class="p1">There are a whole theory and math behind the Big-O notation and other notations related. At this time, just take a look of the typical code and its growth order.</p>


<p class="p1"><img alt="" src="http://adrianmejiarosario.com/sites/default/files/Screen%20Shot%202011-12-22%20at%204.51.48%20PM.png" style="width: 600px; height: 427px; " /></p>


<p>&nbsp;</p>


<p><strong>Cases (the good, the bad, and the ugly)</strong></p>


<p>Remember that n is the number of elements in the input. All this runtime growth rate are in function of the input elements. There is another important thing to consider about the input elements: the order! The order of the input elements matters, and that&#39;s why algorithms are analyzed in 3 different cases:</p>


<ol>
    <li>
        Worst-case performance: the input is distributed as worst as it could be for an algorithm. &nbsp;&nbsp;</li>
    <li>
        Average-case scenario: approximation of the most common arrange of inputs.</li>
    <li>
        Best-case scenario: most favorable distribution of the inputs.</li>
    <li>
        One more: Space. this is how much space the algorithm cosume to execute.&nbsp;</li>
</ol>


<p class="p2">If you want more depth in these topic read here:&nbsp;</p>


<ul>
    <li class="p2">
        <span style="color: rgb(0, 0, 0); font-family: Helvetica, Arial, sans-serif; line-height: 16px; background-color: rgb(255, 255, 255); font-size: small; ">Analysis (</span><a href="http://gcu.googlecode.com/files/02Analysis.pdf" style="color: rgb(85, 26, 139); font-family: Helvetica, Arial, sans-serif; line-height: 16px; background-color: rgb(255, 255, 255); font-size: small; ">pdf</a><span style="color: rgb(0, 0, 0); font-family: Helvetica, Arial, sans-serif; line-height: 16px; background-color: rgb(255, 255, 255); font-size: small; ">) (</span><a href="http://gcu.googlecode.com/files/02Analysis.key.zip" style="color: rgb(85, 26, 139); font-family: Helvetica, Arial, sans-serif; line-height: 16px; background-color: rgb(255, 255, 255); font-size: small; ">keynote</a><span style="color: rgb(0, 0, 0); font-family: Helvetica, Arial, sans-serif; line-height: 16px; background-color: rgb(255, 255, 255); font-size: small; ">)</span></li>
    <li class="p2">
        <span style="background-color: rgb(255, 255, 255); color: rgb(0, 0, 0); font-family: Helvetica, Arial, sans-serif; font-size: small; line-height: 16px; ">Algorithm @&nbsp;</span>ocw.mit.edu: lectures <a href="http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-046j-introduction-to-algorithms-sma-5503-fall-2005/video-lectures/lecture-1-administrivia-introduction-analysis-of-algorithms-insertion-sort-mergesort">1 </a>and <a href="http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-046j-introduction-to-algorithms-sma-5503-fall-2005/video-lectures/lecture-2-asymptotic-notation-recurrences-substitution-master-method">2</a></li>
    <li class="p2">
        http://algs4.cs.princeton.edu/home/</li>
</ul>


<p class="p2">&nbsp;</p>

]]></content>
  </entry>
  
</feed>
